# 连接成功后的操作指南

## 🎉 恭喜！数据库连接成功

现在你可以开始使用告警数据库系统了。以下是接下来的步骤：

## 1. 查看数据库结构

在 DBeaver 中，你应该能看到：

### 数据库 `alert_db`
- **表：`alerts`** - 存储所有告警数据

### alerts 表结构

| 字段名 | 类型 | 说明 |
|--------|------|------|
| id | INTEGER | 主键，自动递增 |
| alert_type | VARCHAR(50) | 告警类型（"告警触发" 或 "告警恢复"） |
| region | VARCHAR(100) | 区域（如 IDN） |
| metric | VARCHAR(100) | 指标（如 ConnectionRate） |
| rule_name | VARCHAR(200) | 规则名称 |
| generator_url | TEXT | 告警链接 |
| alert_summary | TEXT | 告警摘要 |
| alert_details | TEXT | 告警详情 |
| enterprise_name | VARCHAR(200) | 企业名称 |
| script_name | VARCHAR(200) | 话术名称 |
| time | TIMESTAMP | 告警时间 |
| raw_input | TEXT | 原始输入数据 |
| processed | BOOLEAN | 是否已处理 |
| timeout_triggered | BOOLEAN | 是否已触发超时通知 |
| created_at | TIMESTAMP | 创建时间 |
| updated_at | TIMESTAMP | 更新时间 |

## 2. 在 DBeaver 中查看数据

### 查看表结构
1. 展开数据库 `alert_db`
2. 展开 `Schemas` → `public` → `Tables`
3. 找到 `alerts` 表，右键 → **查看数据 (View Data)**

### 执行 SQL 查询
可以运行以下 SQL 查询：

```sql
-- 查看所有告警
SELECT * FROM alerts ORDER BY created_at DESC;

-- 查看告警数量
SELECT COUNT(*) as total_alerts FROM alerts;

-- 按告警类型统计
SELECT alert_type, COUNT(*) as count 
FROM alerts 
GROUP BY alert_type;

-- 按企业统计
SELECT enterprise_name, COUNT(*) as count 
FROM alerts 
GROUP BY enterprise_name 
ORDER BY count DESC;

-- 查看最近的告警
SELECT id, alert_type, enterprise_name, metric, time 
FROM alerts 
ORDER BY time DESC 
LIMIT 10;
```

## 3. 测试 API 创建数据

### 方法一：使用测试脚本

运行项目提供的测试脚本：

```bash
python test_api.py
```

这个脚本会：
- 测试创建"告警触发"消息
- 测试创建"告警恢复"消息
- 查询告警列表
- 查询单个告警详情

### 方法二：使用 API 文档界面

1. 在浏览器打开：`http://localhost:8000/docs`
2. 找到 `POST /api/alert` 接口
3. 点击 "Try it out"
4. 填写请求体，例如：

```json
{
  "input": "🔴 **【告警触发】监控告警**\n🌐 **区域 (Region):** IDN\n📊 **指标 (Metric):** ConnectionRate\n🔍 **规则名称 (Rule Name):** IDN-Enterprise-ConnectionRate\n🔗 **告警链接 (GeneratorURL):** https://monitor.talkbots.cn:443/alerting/grafana/eeze89xvvn7cwd/view?orgId=1\n\n**告警摘要:**\n企业 KrediOne CG 的接通率\n\n**告警详情:**\n在过去十五分钟内的接通率为 14.03%\n呼叫量为 2776\n参考阈值: 0.5%~20%",
  "enterprise_name": "KrediOne CG",
  "time": "2025-12-10 10:25:34"
}
```

5. 点击 "Execute" 发送请求
6. 然后在 DBeaver 中刷新 `alerts` 表，应该能看到新插入的数据

### 方法三：使用 curl 命令

```bash
curl -X POST "http://localhost:8000/api/alert" \
  -H "Content-Type: application/json" \
  -d '{
    "input": "🔴 **【告警触发】监控告警**\n🌐 **区域 (Region):** IDN\n📊 **指标 (Metric):** ConnectionRate\n🔍 **规则名称 (Rule Name):** IDN-Enterprise-ConnectionRate",
    "enterprise_name": "Test Enterprise",
    "time": "2025-12-10 10:25:34"
  }'
```

## 4. 验证数据是否正确写入

在 DBeaver 中运行：

```sql
-- 查看最新插入的告警
SELECT * FROM alerts 
WHERE enterprise_name = 'KrediOne CG'  -- 替换为你测试时使用的企业名
ORDER BY created_at DESC 
LIMIT 1;
```

检查字段是否正确解析：
- `alert_type` 应该是 "告警触发"
- `region` 应该是 "IDN"
- `metric` 应该是 "ConnectionRate"
- `enterprise_name` 应该正确
- `script_name` 应该从告警摘要中提取出来

## 5. 监控超时机制

系统会自动监控"告警触发"消息，如果20分钟内没有收到同企业、同话术、同告警类型的"告警触发"，会触发超时通知。

### 查看超时状态

```sql
-- 查看未触发超时的告警触发
SELECT id, enterprise_name, script_name, metric, time, timeout_triggered
FROM alerts
WHERE alert_type = '告警触发'
  AND timeout_triggered = false
ORDER BY time DESC;
```

### 测试超时机制（需要等待20分钟）

1. 创建一个"告警触发"消息
2. 等待20分钟（或修改配置缩短测试时间）
3. 检查 `timeout_triggered` 字段是否变为 `true`
4. 检查日志查看是否调用了超时 Dify workflow API

## 6. 常用操作

### 清空测试数据（谨慎使用）

```sql
-- 删除所有数据
DELETE FROM alerts;

-- 重置自增ID（PostgreSQL）
ALTER SEQUENCE alerts_id_seq RESTART WITH 1;
```

### 查看数据库统计

```sql
-- 完整统计
SELECT 
    COUNT(*) as total,
    COUNT(CASE WHEN alert_type = '告警触发' THEN 1 END) as triggers,
    COUNT(CASE WHEN alert_type = '告警恢复' THEN 1 END) as recoveries,
    COUNT(CASE WHEN timeout_triggered = true THEN 1 END) as timeouts,
    MIN(time) as earliest_alert,
    MAX(time) as latest_alert
FROM alerts;
```

## 7. 下一步

现在系统已经准备好了！你可以：

1. **集成到 Dify workflow**
   - 配置 Dify workflow 调用 `http://localhost:8000/api/alert` 接口
   - 确保设置正确的 `DIFY_WEBHOOK_URL_TIMEOUT` 用于超时通知

2. **配置环境变量**
   - 编辑 `.env` 文件
   - 设置 `DIFY_WEBHOOK_URL_TIMEOUT` 为你实际的 Dify workflow webhook

3. **监控和调试**
   - 使用 DBeaver 实时查看数据
   - 使用 `docker-compose logs -f backend` 查看后端日志
   - 使用 API 文档测试接口

## 8. 查看日志

```bash
# 查看后端服务日志
docker-compose logs -f backend

# 查看数据库日志
docker-compose logs -f postgres

# 查看所有服务日志
docker-compose logs -f
```

---

**提示：** 现在可以开始接收真实的告警数据了！系统会自动处理数据的存储、解析和超时监控。



